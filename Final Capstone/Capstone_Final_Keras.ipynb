{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Capstone: Tensor Flow and Keras Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction \n",
    "\n",
    "The following is a dataset from articles from 15 American publications. It has 3 sets of data with articles that was scraped on the internet. But we only use one that contains 50,000 news articles. \n",
    "\n",
    "\n",
    "* The articles1 dataset  contains:     \n",
    "    * id    \n",
    "    * title                 \n",
    "    * publication               \n",
    "    * author        \n",
    "    * date        \n",
    "    * year         \n",
    "    * month                 \n",
    "    * url                                                             \n",
    "    * content (full articles)   \n",
    "    \n",
    "        \n",
    " \n",
    "The original source can be found [here](https://www.kaggle.com/snapcrack/all-the-news#articles1.csv)\n",
    "\n",
    "\n",
    "For this research we only looked at the aricles __titles__ and __contents__  \n",
    "\n",
    "\n",
    "# Research Interest   \n",
    "\n",
    "__Goal:__ To build a model that can help us put together articles with the same __Topics__ together. The articles __Titles__ are used to figure out the different articles topics.      \n",
    "Then use our model to be able to predict what topic a new article would fall under based on its content.    \n",
    "\n",
    "\n",
    "The project has been divided into:\n",
    "\n",
    "1. Loading libraries\n",
    "\n",
    "2. Importing Data\n",
    "\n",
    "3. Resampling Data\n",
    "\n",
    "4. Cleaning Data\n",
    "\n",
    "5. Creating Training and Test Sets\n",
    "\n",
    "6. Create Features Using tf-idf\n",
    "\n",
    "7. Clustering of Titles of articles in the Dataset using LSA.\n",
    "      * LSA using 10 Topics \n",
    "      * LSA using 20 Topics\n",
    "      * LSA using 3 Topis\n",
    "      \n",
    "10. Modeling With Keras \n",
    "      * Keras with 10 articles  \n",
    "      * Keras with 20 articles\n",
    "      * Keras with 3 articles \n",
    "    \n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset:[15 Americans publications](https://www.kaggle.com/snapcrack/all-the-news#articles3.csv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python3.6/importlib/_bootstrap.py:205: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n",
      "/anaconda/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import keras\n",
    "import gc\n",
    "\n",
    "import time\n",
    "\n",
    "import warnings\n",
    "# Suppress annoying harmless error.\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "from IPython.display import Image\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import the dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Import various componenets for model building\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import LSTM, Input, TimeDistributed\n",
    "from keras.models import Model\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "\n",
    "\n",
    "from keras.layers import Convolution1D, MaxPooling1D, Flatten\n",
    "\n",
    "\n",
    "# Import the backend\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('articles1.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resampling the Data Frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Remove article to use for prediction\n",
    "df_new = df.sample(n=1, replace=False, axis = 0, random_state=20)\n",
    "rem = df_new.index\n",
    "X_new = df['content'][rem[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_new = pd.Series(X_new, index=rem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18991    BERLIN (Reuters)  —   Tens of thousands of peo...\n",
       "dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(X_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.drop(rem, axis=0, inplace = True)\n",
    "df1 = df.sample(frac=0.2, replace=False, axis = 0, random_state=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#delete df to clear up space in the memory\n",
    "\n",
    "del df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rem[0]  in df1.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18991    BERLIN (Reuters)  —   Tens of thousands of peo...\n",
       "dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the data in articles 1 is: (10000, 10)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>publication</th>\n",
       "      <th>author</th>\n",
       "      <th>date</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>url</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10140</th>\n",
       "      <td>10140</td>\n",
       "      <td>28876</td>\n",
       "      <td>’The View’ Co-Hosts Ask If Palin, Nugent, Kid ...</td>\n",
       "      <td>Breitbart</td>\n",
       "      <td>Pam Key</td>\n",
       "      <td>2017-04-21</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Friday on ABC’s “The View,” the panel discusse...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31612</th>\n",
       "      <td>31624</td>\n",
       "      <td>50390</td>\n",
       "      <td>Istanbul attack: ISIS claims nightclub shootin...</td>\n",
       "      <td>CNN</td>\n",
       "      <td>Euan McKirdy</td>\n",
       "      <td>2017-01-02</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Istanbul (CNN) ISIS claimed responsibility for...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7077</th>\n",
       "      <td>7077</td>\n",
       "      <td>25574</td>\n",
       "      <td>Trump’s Breezy Calls to World Leaders Leave Di...</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>Mark Landler</td>\n",
       "      <td>2017-04-14</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>WASHINGTON  —     Donald J. Trump inherited a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35657</th>\n",
       "      <td>36455</td>\n",
       "      <td>55281</td>\n",
       "      <td>Trump campaign: We’re facing an emergency goal...</td>\n",
       "      <td>CNN</td>\n",
       "      <td>Eugene Scott</td>\n",
       "      <td>2016-06-18</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Washington (CNN) The Donald Trump campaign on ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8449</th>\n",
       "      <td>8449</td>\n",
       "      <td>27185</td>\n",
       "      <td>Scientists Turn Spinach Leaf into Beating Huma...</td>\n",
       "      <td>Breitbart</td>\n",
       "      <td>Jack Hadfield</td>\n",
       "      <td>2017-03-30</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Researchers at Worcester Polytechnic Institute...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0     id                                              title  \\\n",
       "10140       10140  28876  ’The View’ Co-Hosts Ask If Palin, Nugent, Kid ...   \n",
       "31612       31624  50390  Istanbul attack: ISIS claims nightclub shootin...   \n",
       "7077         7077  25574  Trump’s Breezy Calls to World Leaders Leave Di...   \n",
       "35657       36455  55281  Trump campaign: We’re facing an emergency goal...   \n",
       "8449         8449  27185  Scientists Turn Spinach Leaf into Beating Huma...   \n",
       "\n",
       "          publication         author        date    year  month  url  \\\n",
       "10140       Breitbart        Pam Key  2017-04-21  2017.0    4.0  NaN   \n",
       "31612             CNN   Euan McKirdy  2017-01-02  2017.0    1.0  NaN   \n",
       "7077   New York Times   Mark Landler  2017-04-14  2017.0    4.0  NaN   \n",
       "35657             CNN   Eugene Scott  2016-06-18  2016.0    6.0  NaN   \n",
       "8449        Breitbart  Jack Hadfield  2017-03-30  2017.0    3.0  NaN   \n",
       "\n",
       "                                                 content  \n",
       "10140  Friday on ABC’s “The View,” the panel discusse...  \n",
       "31612  Istanbul (CNN) ISIS claimed responsibility for...  \n",
       "7077   WASHINGTON  —     Donald J. Trump inherited a ...  \n",
       "35657  Washington (CNN) The Donald Trump campaign on ...  \n",
       "8449   Researchers at Worcester Polytechnic Institute...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('The shape of the data in articles 1 is:', df1.shape)\n",
    "display(df1.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'id', 'title', 'publication', 'author', 'date', 'year',\n",
       "       'month', 'url', 'content'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "author     1260\n",
       "url       10000\n",
       "dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count nulls \n",
    "null_count = df1.isnull().sum()\n",
    "null_count[null_count>0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean the Article Content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10140    Friday on ABC’s “The View,” the panel discusse...\n",
       "Name: content, dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1['content'].head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Here we clean the content by removing all the  punctuation, \n",
    "#removing all that is unnecessary.\n",
    "\n",
    "df1['content'] = df1['content'].str.replace(r'[^a-zA-Z0-9 ]', \"\",).fillna('')\n",
    "df1['content'] = df1['content'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10140    friday on abcs the view the panel discussed th...\n",
       "Name: content, dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1['content'].head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean the Article Title "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Breitbart           4727\n",
       "CNN                 2310\n",
       "New York Times      1556\n",
       "Business Insider    1370\n",
       "Atlantic              37\n",
       "Name: publication, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List of Publications to remove in titles\n",
    "\n",
    "df1['publication'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10140    ’The View’ Co-Hosts Ask If Palin, Nugent, Kid ...\n",
       "Name: title, dtype: object"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1['title'].head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df1['title'] = df1['title'].str.replace(r'Breitbart|CNN|The New York Times|Business Insider|Atlantic', \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Here we clean the Title by removing all the  punctuation, \n",
    "#removing all that is unnecessary.\n",
    "\n",
    "df1['title'] = df1['title'].str.replace(r'[^a-zA-Z0-9 ]', \"\",).fillna('')\n",
    "df1['title'] = df1['title'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10140    the view cohosts ask if palin nugent kid rock ...\n",
       "Name: title, dtype: object"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1['title'].head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = df1[['title', 'publication', 'author', 'date', 'year','month', 'content']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#delete df1 to clear up space in the memory\n",
    "\n",
    "del df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the data in articles 1 is: (10000, 7)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>publication</th>\n",
       "      <th>author</th>\n",
       "      <th>date</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10140</th>\n",
       "      <td>the view cohosts ask if palin nugent kid rock ...</td>\n",
       "      <td>Breitbart</td>\n",
       "      <td>Pam Key</td>\n",
       "      <td>2017-04-21</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>friday on abcs the view the panel discussed th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31612</th>\n",
       "      <td>istanbul attack isis claims nightclub shooting...</td>\n",
       "      <td>CNN</td>\n",
       "      <td>Euan McKirdy</td>\n",
       "      <td>2017-01-02</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>istanbul cnn isis claimed responsibility for t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7077</th>\n",
       "      <td>trumps breezy calls to world leaders leave dip...</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>Mark Landler</td>\n",
       "      <td>2017-04-14</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>washington       donald j trump inherited a co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35657</th>\n",
       "      <td>trump campaign were facing an emergency goal o...</td>\n",
       "      <td>CNN</td>\n",
       "      <td>Eugene Scott</td>\n",
       "      <td>2016-06-18</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>washington cnn the donald trump campaign on sa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8449</th>\n",
       "      <td>scientists turn spinach leaf into beating huma...</td>\n",
       "      <td>Breitbart</td>\n",
       "      <td>Jack Hadfield</td>\n",
       "      <td>2017-03-30</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>researchers at worcester polytechnic institute...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   title     publication  \\\n",
       "10140  the view cohosts ask if palin nugent kid rock ...       Breitbart   \n",
       "31612  istanbul attack isis claims nightclub shooting...             CNN   \n",
       "7077   trumps breezy calls to world leaders leave dip...  New York Times   \n",
       "35657  trump campaign were facing an emergency goal o...             CNN   \n",
       "8449   scientists turn spinach leaf into beating huma...       Breitbart   \n",
       "\n",
       "              author        date    year  month  \\\n",
       "10140        Pam Key  2017-04-21  2017.0    4.0   \n",
       "31612   Euan McKirdy  2017-01-02  2017.0    1.0   \n",
       "7077    Mark Landler  2017-04-14  2017.0    4.0   \n",
       "35657   Eugene Scott  2016-06-18  2016.0    6.0   \n",
       "8449   Jack Hadfield  2017-03-30  2017.0    3.0   \n",
       "\n",
       "                                                 content  \n",
       "10140  friday on abcs the view the panel discussed th...  \n",
       "31612  istanbul cnn isis claimed responsibility for t...  \n",
       "7077   washington       donald j trump inherited a co...  \n",
       "35657  washington cnn the donald trump campaign on sa...  \n",
       "8449   researchers at worcester polytechnic institute...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('The shape of the data in articles 1 is:', df.shape)\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Training and Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y = df['title']\n",
    "X = df[['publication','content']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#  Create Training and Test Sets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.25, random_state=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels = Y_train\n",
    "true_k = np.unique(labels).shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Features Using tf-idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Vectorizer for articles contents "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting features from the contents of articles in dataset using a vectorizer\n",
      "\n",
      "Xvectorizer on articles contents in dataset done in 48.544861 seconds\n",
      "\n",
      "The shape of X_train_tfidf for articles contents is: (7500, 338978)\n",
      "\n",
      "The shape of X_test_tfidf for articles content is: (2500, 338978)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "print(\"Extracting features from the contents of articles in dataset using a vectorizer\")\n",
    "t0 = time.clock()\n",
    "Xvectorizer = TfidfVectorizer(max_df=.5, # drop words that occur in more than half the paragraphs\n",
    "                             min_df=2, # only use words that appear at least twice\n",
    "                             stop_words='english', \n",
    "                             use_idf=True,#we definitely want to use inverse document frequencies in our weighting\n",
    "                             norm=u'l2', #Applies a correction factor so that longer paragraphs and shorter paragraphs get treated equally\n",
    "                             smooth_idf=True, #Adds 1 to all document frequencies, as if an extra document existed that used every word once.  Prevents divide-by-zero errors\n",
    "                             ngram_range=(1, 3)\n",
    "                             )\n",
    "\n",
    "#Find Vocab words on the whole articles \n",
    "#Applying the vectorizer to X_train and X_test\n",
    "X_train_tfidf=Xvectorizer.fit_transform(X_train['content'])\n",
    "X_test_tfidf=Xvectorizer.transform(X_test['content'])\n",
    "vocab = Xvectorizer.vocabulary_\n",
    "\n",
    "print('\\nXvectorizer on articles contents in dataset done in '+'%s seconds'% (time.clock() - t0))\n",
    "\n",
    "\n",
    "print('\\nThe shape of X_train_tfidf for articles contents is:', X_train_tfidf.shape)\n",
    "print('\\nThe shape of X_test_tfidf for articles content is:', X_test_tfidf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del X_train, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Vectorizer for articles titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting features from the titles of articles in dataset using a vectorizer\n",
      "\n",
      "Yvectorizer on articles titles done in 0.7579289999999972 seconds\n",
      "\n",
      "The shape of Y_train_tfidf for articles titles is: (7500, 338978)\n",
      "\n",
      "The shape of Y_test_tfidf for articles titles is: (2500, 338978)\n"
     ]
    }
   ],
   "source": [
    "## Find vectorizer for titles and title and see what kind of vectorizer I need to use for each \n",
    "#(Countvectorizer)\n",
    "\n",
    "print(\"Extracting features from the titles of articles in dataset using a vectorizer\")\n",
    "t0 = time.clock()\n",
    "Yvectorizer = TfidfVectorizer(min_df=2, # only use words that appear at least twice\n",
    "                             stop_words='english', \n",
    "                             use_idf=False,#we definitely want to use inverse document frequencies in our weighting\n",
    "                             norm=u'l2', #Applies a correction factor so that longer paragraphs and shorter paragraphs get treated equally\n",
    "                             smooth_idf=True, #Adds 1 to all document frequencies, as if an extra document existed that used every word once.  Prevents divide-by-zero errors\n",
    "                             vocabulary=vocab,\n",
    "                             ngram_range=(1, 3)\n",
    "                             )\n",
    "\n",
    "#Applying the vectorizer to Y_train and Y_test\n",
    "\n",
    "Y_train_tfidf=Yvectorizer.fit_transform(Y_train)\n",
    "Y_test_tfidf=Yvectorizer.transform(Y_test)\n",
    "print('\\nYvectorizer on articles titles done in '+'%s seconds'% (time.clock() - t0))\n",
    "\n",
    "print('\\nThe shape of Y_train_tfidf for articles titles is:', Y_train_tfidf.shape)\n",
    "print('\\nThe shape of Y_test_tfidf for articles titles is:', Y_test_tfidf.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Vectorizer for X_new\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Here we clean the content by removing all the  punctuation, \n",
    "#removing all that is unnecessary.\n",
    "\n",
    "X_new = X_new.str.replace(r'[^a-zA-Z0-9 ]', \"\",).fillna('')\n",
    "X_new = X_new.str.lower()\n",
    "X_new = X_new.str.replace(r'Breitbart|CNN|The New York Times|Business Insider|Atlantic', \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18991    berlin reuters     tens of thousands of people...\n",
       "dtype: object"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The shape of X_new_tfidf for articles content is: (1, 338978)\n"
     ]
    }
   ],
   "source": [
    "#Find Vocab words on the whole articles \n",
    "#Applying the vectorizer to X_news and X_test\n",
    "X_new_tfidf=Xvectorizer.transform(X_new)\n",
    "print('\\nThe shape of X_new_tfidf for articles content is:', X_new_tfidf.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering of titles of articles in dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSA on the titles of the articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSA on the titles of the articles  10 articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSA for 10 articles done in 2.6020189999999985 seconds\n",
      "\n",
      "Percent variance captured by all components:  5.320400698615563\n"
     ]
    }
   ],
   "source": [
    "#Our SVD data reducer.  We are going to reduce the feature space from 84939 to 1000.\n",
    "t0 = time.clock()\n",
    "svd= TruncatedSVD(10, random_state = 20)\n",
    "lsa = make_pipeline(svd, Normalizer(copy=False))\n",
    "\n",
    "\n",
    "# Run SVD on the training data, then project the training data.\n",
    "Y_train_lsa10 = lsa.fit_transform(Y_train_tfidf)\n",
    "Y_test_lsa10 = lsa.transform(Y_test_tfidf)\n",
    "\n",
    "print('LSA for 10 articles done in '+'%s seconds'% (time.clock() - t0))\n",
    "\n",
    "variance_explained=svd.explained_variance_ratio_\n",
    "total_variance = variance_explained.sum()\n",
    "print('\\nPercent variance captured by all components: ', (total_variance*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of Y_train_lsa for titles is: (7500, 10)\n",
      "The shape of Y_test_lsa for titles is: (2500, 10)\n"
     ]
    }
   ],
   "source": [
    "print('The shape of Y_train_lsa for titles is:', Y_train_lsa10.shape)\n",
    "print('The shape of Y_test_lsa for titles is:', Y_test_lsa10.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#What are the topics in the articles looking at the biggest components articles topics.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the Training set:\n",
      "Component 0:\n",
      "30778    0.966490\n",
      "47148    0.965407\n",
      "27063    0.963733\n",
      "Name: 0, dtype: float64\n",
      "Component 1:\n",
      "5016     0.981777\n",
      "23181    0.981603\n",
      "28540    0.981310\n",
      "Name: 1, dtype: float64\n",
      "Component 2:\n",
      "17696    0.883292\n",
      "22873    0.876342\n",
      "34745    0.796539\n",
      "Name: 2, dtype: float64\n",
      "Component 3:\n",
      "46001    0.986534\n",
      "661      0.980011\n",
      "45155    0.979980\n",
      "Name: 3, dtype: float64\n",
      "Component 4:\n",
      "10409    0.937106\n",
      "46118    0.933026\n",
      "7571     0.926196\n",
      "Name: 4, dtype: float64\n",
      "\n",
      "For the Test set:\n",
      "Component 0:\n",
      "25624    0.969691\n",
      "1868     0.961950\n",
      "17290    0.960716\n",
      "Name: 0, dtype: float64\n",
      "Component 1:\n",
      "9481     0.980387\n",
      "44518    0.979890\n",
      "30089    0.979770\n",
      "Name: 1, dtype: float64\n",
      "Component 2:\n",
      "32970    0.797566\n",
      "33895    0.796773\n",
      "25871    0.796704\n",
      "Name: 2, dtype: float64\n",
      "Component 3:\n",
      "49561    0.978538\n",
      "45513    0.977176\n",
      "31622    0.974661\n",
      "Name: 3, dtype: float64\n",
      "Component 4:\n",
      "40577    0.901852\n",
      "32044    0.901683\n",
      "670      0.896164\n",
      "Name: 4, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#Looking at what sorts of titles our solution considers similar, for the first five identified topics\n",
    "print('For the Training set:')\n",
    "titles1_by_component=pd.DataFrame(Y_train_lsa10,index=Y_train.index)\n",
    "for i in range(5):\n",
    "    print('Component {}:'.format(i))\n",
    "    print(titles1_by_component.loc[:,i].sort_values(ascending=False)[0:3])\n",
    "    \n",
    "print('\\nFor the Test set:')    \n",
    "\n",
    "titles2_by_component=pd.DataFrame(Y_test_lsa10,index=Y_test.index)\n",
    "for i in range(5):\n",
    "    print('Component {}:'.format(i))\n",
    "    print(titles2_by_component.loc[:,i].sort_values(ascending=False)[0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Component_train10 = pd.DataFrame()\n",
    "Component_train10['title'] = Y_train\n",
    "Component_train10['component'] = titles1_by_component.idxmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Component_test10 = pd.DataFrame()\n",
    "Component_test10['title'] = Y_test\n",
    "Component_test10['component'] = titles2_by_component.idxmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y_train_component10 = Component_train10['component']\n",
    "Y_test_component10 = Component_test10['component']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>component</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>27396</th>\n",
       "      <td>trump warns hillary wants to abolish the secon...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49747</th>\n",
       "      <td>the 11 best laptops of 2016</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26513</th>\n",
       "      <td>team of grifters tim kaine reinforces crooked ...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19619</th>\n",
       "      <td>investigation migrants smuggled into uk posing...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1073</th>\n",
       "      <td>a long way from mexico company bets china has ...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   title  component\n",
       "27396  trump warns hillary wants to abolish the secon...          0\n",
       "49747                        the 11 best laptops of 2016          9\n",
       "26513  team of grifters tim kaine reinforces crooked ...          6\n",
       "19619  investigation migrants smuggled into uk posing...          9\n",
       "1073   a long way from mexico company bets china has ...          9"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Component_train10.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>component</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>27594</th>\n",
       "      <td>clinton vp pick tim kaines islamist ties</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21699</th>\n",
       "      <td>report donald trump no show at colorado state ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40577</th>\n",
       "      <td>generous kidney donor triggers 6 transplants</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28470</th>\n",
       "      <td>zumwalt fifteen years after 911 what have we l...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24395</th>\n",
       "      <td>texas prisoners bust out of jail  to save jailer</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   title  component\n",
       "27594         clinton vp pick tim kaines islamist ties            1\n",
       "21699  report donald trump no show at colorado state ...          0\n",
       "40577       generous kidney donor triggers 6 transplants          4\n",
       "28470  zumwalt fifteen years after 911 what have we l...          9\n",
       "24395   texas prisoners bust out of jail  to save jailer          9"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Component_test10.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Articles in Component  0\n",
      "                                                   title  component\n",
      "30778  sad leftists protest donald trump election at ...          0\n",
      "47148  the never trump movement has settled on a cand...          0\n",
      "27063  donald trump spokesman nikki haley had natural...          0\n",
      "\n",
      "Articles in Component  1\n",
      "                                                   title  component\n",
      "5016   russias hacks followed years of paranoia towar...          1\n",
      "23181  hillary clinton next to cardinal dolan at al s...          1\n",
      "28540  hillary clinton to goldman sachs i represented...          1\n",
      "\n",
      "Articles in Component  2\n",
      "                                                   title  component\n",
      "17696  exclusive  the donald endorses the donald rums...          2\n",
      "22873  sheriff joe the donald establishment doesnt wa...          2\n",
      "34745          donald trumps risky religious pilgrimage           2\n",
      "\n",
      "Articles in Component  3\n",
      "                                                   title  component\n",
      "46001  a former wall street analyst just set a new wo...          3\n",
      "661    his predecessor gone gambias new president fin...          3\n",
      "45155  senate republicans and democrats agree on new ...          3\n",
      "\n",
      "Articles in Component  4\n",
      "                                                   title  component\n",
      "10409  fed hikes key interest rate signals economic s...          4\n",
      "46118                     tesla isnt disrupting anything          4\n",
      "7571   nasas juno spacecraft enters into orbit around...          4\n",
      "\n",
      "Articles in Component  5\n",
      "                                                   title  component\n",
      "48897  obama to paul ryan us missile strikes on yemen...          5\n",
      "31552  realclearpolitics after 8 years of obama gop s...          5\n",
      "23551  judge jeanine to obama stop focusing on gun co...          5\n",
      "\n",
      "Articles in Component  6\n",
      "                                         title  component\n",
      "35190  terrorist attacks by vehicle fast facts          6\n",
      "35649              aung san suu kyi fast facts          6\n",
      "34000                   stanley cup fast facts          6\n",
      "\n",
      "Articles in Component  7\n",
      "                                                   title  component\n",
      "42789  kid confronts cyberbullies earns white house i...          7\n",
      "23872  cult of harambe demands white house name warsh...          7\n",
      "46770  journalist freed from an iranian prison addres...          7\n",
      "\n",
      "Articles in Component  8\n",
      "                                                   title  component\n",
      "16643  planned parenthood says ted cruz biggest liar ...          8\n",
      "19555  texas lawyer files suit to challenge ted cruzs...          8\n",
      "24331  cruz mocks fox news reporting on rubio looking...          8\n",
      "\n",
      "Articles in Component  9\n",
      "                                                   title  component\n",
      "11038    salk institute creates humanpig hybrid embryo            9\n",
      "17966  police ucla gunman purchased all firearms legally          9\n",
      "34330  lessons from the la riots how a troubled polic...          9\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(\"\\nArticles in Component \", i)\n",
    "    title_index = titles1_by_component.loc[:,i].sort_values(ascending=False)[0:3].index\n",
    "    print(Component_train10.loc[title_index,['title','component']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The shape of Y_train_tfidf for articles titles is: (7500, 338978)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9    2839\n",
       "0    1203\n",
       "4     844\n",
       "8     647\n",
       "3     602\n",
       "1     480\n",
       "5     413\n",
       "7     244\n",
       "6     160\n",
       "2      68\n",
       "Name: component, dtype: int64"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('\\nThe shape of Y_train_tfidf for articles titles is:', Y_train_tfidf.shape)\n",
    "Component_train10.component.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can note that Component 7, 0 and 4 contain the most articles.     \n",
    "But the articles can be seen as well distributed between the different Components. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top terms per components:\n",
      "\n",
      "tf_vectorizer on done in 0.048676000000000386 seconds\n",
      "\n",
      "Topics in Component  0\n",
      "['campaign', 'clinton', 'cruz', 'donald', 'hillary', 'obama', 'poll', 'president', 'says', 'trump']\n",
      "\n",
      "tf_vectorizer on done in 0.06504199999999827 seconds\n",
      "\n",
      "Topics in Component  1\n",
      "['bernie', 'campaign', 'clinton', 'clintons', 'email', 'fbi', 'foundation', 'hillary', 'poll', 'sanders']\n",
      "\n",
      "tf_vectorizer on done in 0.06984900000000493 seconds\n",
      "\n",
      "Topics in Component  2\n",
      "['border', 'briefing', 'change', 'debate', 'donald', 'mike', 'news', 'remarks', 'trumps', 'victory']\n",
      "\n",
      "tf_vectorizer on done in 0.08924499999999824 seconds\n",
      "\n",
      "Topics in Component  3\n",
      "['best', 'california', 'city', 'just', 'new', 'times', 'today', 'world', 'year', 'york']\n",
      "\n",
      "tf_vectorizer on done in 0.12134499999999804 seconds\n",
      "\n",
      "Topics in Component  4\n",
      "['america', 'ban', 'care', 'immigration', 'plan', 'president', 'russia', 'speech', 'trumps', 'wall']\n",
      "\n",
      "tf_vectorizer on done in 0.15159700000000242 seconds\n",
      "\n",
      "Topics in Component  5\n",
      "['administration', 'control', 'dallas', 'democrats', 'gun', 'israel', 'obama', 'syria', 'syrian', 'war']\n",
      "\n",
      "tf_vectorizer on done in 0.1655760000000015 seconds\n",
      "\n",
      "Topics in Component  6\n",
      "['cia', 'court', 'david', 'facts', 'fast', 'game', 'national', 'review', 'supreme', 'young']\n",
      "\n",
      "tf_vectorizer on done in 0.18805799999999806 seconds\n",
      "\n",
      "Topics in Component  7\n",
      "['care', 'gop', 'health', 'house', 'jared', 'kushner', 'obamacare', 'repeal', 'trump', 'white']\n",
      "\n",
      "tf_vectorizer on done in 0.21375799999999856 seconds\n",
      "\n",
      "Topics in Component  8\n",
      "['campaign', 'carolina', 'cruz', 'gop', 'race', 'rubio', 'south', 'ted', 'vote', 'watch']\n",
      "\n",
      "tf_vectorizer on done in 0.31514299999999906 seconds\n",
      "\n",
      "Topics in Component  9\n",
      "['attack', 'border', 'man', 'milo', 'news', 'police', 'report', 'says', 'state', 'texas']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "tf_vectorizer = CountVectorizer(max_features=10,\n",
    "                                stop_words='english')\n",
    "t0 = time.clock()\n",
    "\n",
    "print(\"Top terms per components:\") \n",
    "for i in range(10):\n",
    "    tf = tf_vectorizer.fit_transform(Component_train10.loc[Component_train10['component'] == i,'title'])\n",
    "    print('\\ntf_vectorizer on done in '+'%s seconds'% (time.clock() - t0))\n",
    "    print(\"\\nTopics in Component \", i)\n",
    "    tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "    print(tf_feature_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSA on the titles of the articles  20 articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSA for 20 articles done in 4.388192999999994 seconds\n",
      "\n",
      "Percent variance captured by all components:  7.4334836605652015\n"
     ]
    }
   ],
   "source": [
    "#Our SVD data reducer.  We are going to reduce the feature space from 84939 to 1000.\n",
    "t0 = time.clock()\n",
    "svd= TruncatedSVD(20, random_state = 20)\n",
    "lsa = make_pipeline(svd, Normalizer(copy=False))\n",
    "\n",
    "\n",
    "# Run SVD on the training data, then project the training data.\n",
    "Y_train_lsa20 = lsa.fit_transform(Y_train_tfidf)\n",
    "Y_test_lsa20 = lsa.transform(Y_test_tfidf)\n",
    "\n",
    "print('LSA for 20 articles done in '+'%s seconds'% (time.clock() - t0))\n",
    "\n",
    "variance_explained=svd.explained_variance_ratio_\n",
    "total_variance = variance_explained.sum()\n",
    "print('\\nPercent variance captured by all components: ', (total_variance*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of Y_train_lsa for titles is: (7500, 20)\n",
      "The shape of Y_test_lsa for titles is: (2500, 20)\n"
     ]
    }
   ],
   "source": [
    "print('The shape of Y_train_lsa for titles is:', Y_train_lsa20.shape)\n",
    "print('The shape of Y_test_lsa for titles is:', Y_test_lsa20.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Component 0:\n",
      "30778    0.965064\n",
      "47148    0.964606\n",
      "27063    0.963183\n",
      "Name: 0, dtype: float64\n",
      "Component 1:\n",
      "31043    0.972245\n",
      "41135    0.971826\n",
      "25474    0.971748\n",
      "Name: 1, dtype: float64\n",
      "Component 2:\n",
      "17696    0.880573\n",
      "22873    0.865709\n",
      "34745    0.794662\n",
      "Name: 2, dtype: float64\n",
      "Component 3:\n",
      "34998    0.970703\n",
      "48755    0.970109\n",
      "64       0.968465\n",
      "Name: 3, dtype: float64\n",
      "Component 4:\n",
      "33506    0.887702\n",
      "24177    0.886410\n",
      "43519    0.886227\n",
      "Name: 4, dtype: float64\n",
      "\n",
      "For the Test set:\n",
      "Component 0:\n",
      "1868     0.961388\n",
      "17290    0.933486\n",
      "25624    0.921998\n",
      "Name: 0, dtype: float64\n",
      "Component 1:\n",
      "44518    0.971388\n",
      "14017    0.969291\n",
      "30337    0.968643\n",
      "Name: 1, dtype: float64\n",
      "Component 2:\n",
      "32970    0.796049\n",
      "17838    0.794834\n",
      "33895    0.794819\n",
      "Name: 2, dtype: float64\n",
      "Component 3:\n",
      "25422    0.970657\n",
      "26425    0.968810\n",
      "22847    0.968183\n",
      "Name: 3, dtype: float64\n",
      "Component 4:\n",
      "13992    0.888543\n",
      "18328    0.884994\n",
      "13413    0.883964\n",
      "Name: 4, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#Looking at what sorts of titles our solution considers similar, for the first five identified topics\n",
    "titles1_by_component1=pd.DataFrame(Y_train_lsa20,index=Y_train.index)\n",
    "for i in range(5):\n",
    "    print('Component {}:'.format(i))\n",
    "    print(titles1_by_component1.loc[:,i].sort_values(ascending=False)[0:3])\n",
    "    \n",
    "print('\\nFor the Test set:')    \n",
    "\n",
    "titles2_by_component1=pd.DataFrame(Y_test_lsa20,index=Y_test.index)\n",
    "for i in range(5):\n",
    "    print('Component {}:'.format(i))\n",
    "    print(titles2_by_component1.loc[:,i].sort_values(ascending=False)[0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Looking for the article titles that has the highest component value in the Component cluster. \n",
    "## To see the topic of the article component to see what the topic is actually about.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Component_train20 = pd.DataFrame()\n",
    "Component_train20['title'] = Y_train\n",
    "Component_train20['component'] = titles1_by_component1.idxmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Component_test20 = pd.DataFrame()\n",
    "Component_test20['title'] = Y_test\n",
    "Component_test20['component'] = titles2_by_component1.idxmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y_train_component20 = Component_train20['component']\n",
    "Y_test_component20 = Component_test20['component']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>component</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>27396</th>\n",
       "      <td>trump warns hillary wants to abolish the secon...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49747</th>\n",
       "      <td>the 11 best laptops of 2016</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26513</th>\n",
       "      <td>team of grifters tim kaine reinforces crooked ...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19619</th>\n",
       "      <td>investigation migrants smuggled into uk posing...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1073</th>\n",
       "      <td>a long way from mexico company bets china has ...</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   title  component\n",
       "27396  trump warns hillary wants to abolish the secon...          0\n",
       "49747                        the 11 best laptops of 2016         14\n",
       "26513  team of grifters tim kaine reinforces crooked ...          6\n",
       "19619  investigation migrants smuggled into uk posing...          9\n",
       "1073   a long way from mexico company bets china has ...         19"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Component_train20.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>component</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>27594</th>\n",
       "      <td>clinton vp pick tim kaines islamist ties</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21699</th>\n",
       "      <td>report donald trump no show at colorado state ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40577</th>\n",
       "      <td>generous kidney donor triggers 6 transplants</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28470</th>\n",
       "      <td>zumwalt fifteen years after 911 what have we l...</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24395</th>\n",
       "      <td>texas prisoners bust out of jail  to save jailer</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   title  component\n",
       "27594         clinton vp pick tim kaines islamist ties            1\n",
       "21699  report donald trump no show at colorado state ...          0\n",
       "40577       generous kidney donor triggers 6 transplants          4\n",
       "28470  zumwalt fifteen years after 911 what have we l...         19\n",
       "24395   texas prisoners bust out of jail  to save jailer          9"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Component_test20.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The shape of Y_train_tfidf for articles titles is: (7500, 338978)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "14    1406\n",
       "9     1326\n",
       "0     1146\n",
       "19     449\n",
       "18     430\n",
       "4      365\n",
       "3      363\n",
       "1      356\n",
       "8      255\n",
       "5      246\n",
       "10     230\n",
       "17     217\n",
       "7      178\n",
       "16     129\n",
       "13     106\n",
       "6      104\n",
       "12      92\n",
       "2       59\n",
       "11      32\n",
       "15      11\n",
       "Name: component, dtype: int64"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('\\nThe shape of Y_train_tfidf for articles titles is:', Y_train_tfidf.shape)\n",
    "Component_train20.component.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can note that Components 18,0, 16,7 and 4 contain the most articles. \n",
    "But the articles can be seen as well distributed between the different Components. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top terms per components:\n",
      "\n",
      "tf_vectorizer for 20 articles done in 0.038786000000001764 seconds\n",
      "\n",
      "Topics in Component  0\n",
      "['briefing', 'campaign', 'clinton', 'cruz', 'donald', 'gop', 'hillary', 'like', 'media', 'new', 'obama', 'poll', 'president', 'rally', 'report', 'republicans', 'ryan', 'says', 'supporters', 'trump']\n",
      "\n",
      "tf_vectorizer for 20 articles done in 0.05235100000000159 seconds\n",
      "\n",
      "Topics in Component  1\n",
      "['bernie', 'campaign', 'cash', 'clinton', 'clintons', 'email', 'emails', 'exclusive', 'fbi', 'going', 'hillary', 'new', 'poll', 'press', 'sanders', 'say', 'state', 'trump', 'watch', 'wikileaks']\n",
      "\n",
      "tf_vectorizer for 20 articles done in 0.06332299999999691 seconds\n",
      "\n",
      "Topics in Component  2\n",
      "['border', 'budget', 'calls', 'campaign', 'change', 'debate', 'donald', 'exclusive', 'nationalists', 'news', 'nominee', 'order', 'path', 'pence', 'plan', 'president', 'says', 'speech', 'trumps', 'victory']\n",
      "\n",
      "tf_vectorizer for 20 articles done in 0.07594500000000437 seconds\n",
      "\n",
      "Topics in Component  3\n",
      "['best', 'bombing', 'city', 'eve', 'faces', 'home', 'interview', 'jersey', 'just', 'new', 'obama', 'people', 'prison', 'set', 'times', 'trump', 'trumps', 'year', 'years', 'york']\n",
      "\n",
      "tf_vectorizer for 20 articles done in 0.09348799999999358 seconds\n",
      "\n",
      "Topics in Component  4\n",
      "['america', 'ban', 'calls', 'comments', 'election', 'federal', 'free', 'government', 'immigration', 'inauguration', 'john', 'order', 'people', 'pick', 'secretary', 'speech', 'travel', 'trumps', 'wall', 'washington']\n",
      "\n",
      "tf_vectorizer for 20 articles done in 0.11267899999999997 seconds\n",
      "\n",
      "Topics in Component  5\n",
      "['administration', 'barack', 'ben', 'dallas', 'deal', 'iran', 'legacy', 'merkel', 'michelle', 'need', 'obama', 'president', 'push', 'refugees', 'right', 'says', 'speech', 'syria', 'syrian', 'war']\n",
      "\n",
      "tf_vectorizer for 20 articles done in 0.12490700000000743 seconds\n",
      "\n",
      "Topics in Component  6\n",
      "['attacks', 'cia', 'david', 'facts', 'fast', 'furious', 'game', 'hacking', 'kaine', 'letterman', 'march', 'murray', 'national', 'review', 'sessions', 'shadow', 'spielberg', 'steven', 'tim', 'young']\n",
      "\n",
      "tf_vectorizer for 20 articles done in 0.13360000000000127 seconds\n",
      "\n",
      "Topics in Component  7\n",
      "['bannon', 'black', 'calls', 'caucus', 'freedom', 'gop', 'health', 'house', 'jared', 'kushner', 'leaving', 'obama', 'obamacare', 'patriots', 'people', 'republicans', 'steve', 'trump', 'trumps', 'white']\n",
      "\n",
      "tf_vectorizer for 20 articles done in 0.14498399999999378 seconds\n",
      "\n",
      "Topics in Component  8\n",
      "['abortion', 'announces', 'army', 'ben', 'born', 'carson', 'conservative', 'cruz', 'deal', 'parenthood', 'planned', 'questions', 'race', 'rubio', 'super', 'team', 'ted', 'trump', 'trying', 'women']\n",
      "\n",
      "tf_vectorizer for 20 articles done in 0.225482999999997 seconds\n",
      "\n",
      "Topics in Component  9\n",
      "['arrested', 'attack', 'border', 'girl', 'illegal', 'isis', 'killed', 'man', 'muslim', 'police', 'say', 'says', 'sex', 'shooting', 'syria', 'terror', 'texas', 'video', 'woman', 'women']\n",
      "\n",
      "tf_vectorizer for 20 articles done in 0.24586600000000658 seconds\n",
      "\n",
      "Topics in Component  10\n",
      "['2016', 'daily', 'day', 'election', 'facebook', 'fake', 'fox', 'google', 'hate', 'live', 'media', 'michael', 'milo', 'news', 'social', 'story', 'student', 'sunday', 'twitter', 'yiannopoulos']\n",
      "\n",
      "tf_vectorizer for 20 articles done in 0.25449199999999905 seconds\n",
      "\n",
      "Topics in Component  11\n",
      "['chelsea', 'clinton', 'foundation', 'listen', 'lynch', 'mannings', 'meeting', 'money', 'open', 'percent', 'priebus', 'rant', 'reaction', 'realizing', 'regrets', 'remake', 'returns', 'says', 'shut', 'youre']\n",
      "\n",
      "tf_vectorizer for 20 articles done in 0.26542600000000505 seconds\n",
      "\n",
      "Topics in Component  12\n",
      "['angeles', 'arrested', 'capture', 'charlotte', 'chief', 'close', 'department', 'fatal', 'home', 'islamic', 'justice', 'los', 'officer', 'police', 'release', 'shooting', 'state', 'suspect', 'university', 'video']\n",
      "\n",
      "tf_vectorizer for 20 articles done in 0.2763010000000037 seconds\n",
      "\n",
      "Topics in Component  13\n",
      "['blue', 'blues', 'declares', 'emergency', 'fighters', 'iraq', 'islamic', 'jihadis', 'leader', 'media', 'milo', 'mosul', 'refugees', 'report', 'rex', 'secretary', 'state', 'tillerson', 'turkey', 'university']\n",
      "\n",
      "tf_vectorizer for 20 articles done in 0.3515960000000007 seconds\n",
      "\n",
      "Topics in Component  14\n",
      "['america', 'american', 'big', 'care', 'day', 'dead', 'debate', 'democrats', 'gop', 'gun', 'health', 'heres', 'just', 'make', 'obamacare', 'party', 'plan', 'vote', 'want', 'years']\n",
      "\n",
      "tf_vectorizer for 20 articles done in 0.35672300000000234 seconds\n",
      "\n",
      "Topics in Component  15\n",
      "['change', 'charlotte', 'iranian', 'leaders', 'lochte', 'mix', 'moving', 'nba', 'nc', 'needs', 'nominee', 'oscars', 'pastor', 'paul', 'people', 'prayers', 'present', 'rand', 'rnc', 'ryan']\n",
      "\n",
      "tf_vectorizer for 20 articles done in 0.36354199999999537 seconds\n",
      "\n",
      "Topics in Component  16\n",
      "['bernie', 'campaign', 'democratic', 'dilma', 'king', 'national', 'nevada', 'obamas', 'political', 'poll', 'rally', 'reject', 'run', 'sanders', 'taking', 'trail', 'twitter', 'voters', 'week', 'win']\n",
      "\n",
      "tf_vectorizer for 20 articles done in 0.3722389999999933 seconds\n",
      "\n",
      "Topics in Component  17\n",
      "['american', 'carolina', 'china', 'court', 'korea', 'korean', 'launch', 'like', 'military', 'missile', 'north', 'nuclear', 'ready', 'sea', 'south', 'strike', 'supreme', 'test', 'war', 'world']\n",
      "\n",
      "tf_vectorizer for 20 articles done in 0.38747299999999996 seconds\n",
      "\n",
      "Topics in Component  18\n",
      "['americans', 'attacks', 'california', 'change', 'climate', 'comey', 'haram', 'israel', 'judge', 'justin', 'live', 'man', 'probe', 'report', 'russia', 'russian', 'shows', 'star', 'trailer', 'watch']\n",
      "\n",
      "tf_vectorizer for 20 articles done in 0.4017290000000031 seconds\n",
      "\n",
      "Topics in Component  19\n",
      "['best', 'black', 'border', 'china', 'death', 'dies', 'like', 'lives', 'man', 'matter', 'meet', 'million', 'people', 'photos', 'president', 'street', 'wall', 'world', 'worst', 'years']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "tf_vectorizer = CountVectorizer(max_features=20,\n",
    "                                stop_words='english')\n",
    "t0 = time.clock()\n",
    "\n",
    "print(\"Top terms per components:\") \n",
    "for i in range(20):\n",
    "    tf = tf_vectorizer.fit_transform(Component_train20.loc[Component_train20['component'] == i,'title'])\n",
    "    print('\\ntf_vectorizer for 20 articles done in '+'%s seconds'% (time.clock() - t0))\n",
    "    print(\"\\nTopics in Component \", i)\n",
    "    tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "    print(tf_feature_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSA on the titles of the articles  3 articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSA for 3 articles done in 1.529753999999997 seconds\n",
      "\n",
      "Percent variance captured by all components:  2.909602843418219\n"
     ]
    }
   ],
   "source": [
    "#Our SVD data reducer.  We are going to reduce the feature space from 84939 to 1000.\n",
    "t0 = time.clock()\n",
    "svd= TruncatedSVD(3, random_state = 20)\n",
    "lsa = make_pipeline(svd, Normalizer(copy=False))\n",
    "\n",
    "\n",
    "# Run SVD on the training data, then project the training data.\n",
    "Y_train_lsa3 = lsa.fit_transform(Y_train_tfidf)\n",
    "Y_test_lsa3 = lsa.transform(Y_test_tfidf)\n",
    "\n",
    "print('LSA for 3 articles done in '+'%s seconds'% (time.clock() - t0))\n",
    "\n",
    "variance_explained=svd.explained_variance_ratio_\n",
    "total_variance = variance_explained.sum()\n",
    "print('\\nPercent variance captured by all components: ', (total_variance*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of Y_train_lsa for titles is: (7500, 3)\n",
      "The shape of Y_test_lsa for titles is: (2500, 3)\n"
     ]
    }
   ],
   "source": [
    "print('The shape of Y_train_lsa for titles is:', Y_train_lsa3.shape)\n",
    "print('The shape of Y_test_lsa for titles is:', Y_test_lsa3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the Training set:\n",
      "Component 0:\n",
      "9830    0.999910\n",
      "9850    0.999865\n",
      "7399    0.999764\n",
      "Name: 0, dtype: float64\n",
      "Component 1:\n",
      "33160    0.993754\n",
      "36485    0.993425\n",
      "34657    0.993155\n",
      "Name: 1, dtype: float64\n",
      "Component 2:\n",
      "47668    0.995426\n",
      "46690    0.994464\n",
      "10779    0.990435\n",
      "Name: 2, dtype: float64\n",
      "\n",
      "For the Test set:\n",
      "Component 0:\n",
      "4648     0.999649\n",
      "12953    0.999194\n",
      "48147    0.999181\n",
      "Name: 0, dtype: float64\n",
      "Component 1:\n",
      "36330    0.995073\n",
      "35617    0.993866\n",
      "46597    0.987832\n",
      "Name: 1, dtype: float64\n",
      "Component 2:\n",
      "40437    0.993748\n",
      "32002    0.989788\n",
      "19199    0.989488\n",
      "Name: 2, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#Looking at what sorts of titles our solution considers similar, for the first five identified topics\n",
    "print('For the Training set:')\n",
    "titles1_by_component2=pd.DataFrame(Y_train_lsa3,index=Y_train.index)\n",
    "for i in range(3):\n",
    "    print('Component {}:'.format(i))\n",
    "    print(titles1_by_component2.loc[:,i].sort_values(ascending=False)[0:3])\n",
    "    \n",
    "print('\\nFor the Test set:')    \n",
    "\n",
    "titles2_by_component2=pd.DataFrame(Y_test_lsa3,index=Y_test.index)\n",
    "for i in range(3):\n",
    "    print('Component {}:'.format(i))\n",
    "    print(titles2_by_component2.loc[:,i].sort_values(ascending=False)[0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Component_train3 = pd.DataFrame()\n",
    "Component_train3['title'] = Y_train\n",
    "Component_train3['component'] = titles1_by_component2.idxmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Component_test3 = pd.DataFrame()\n",
    "Component_test3['title'] = Y_test\n",
    "Component_test3['component'] = titles2_by_component2.idxmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y_train_component3 = Component_train3['component']\n",
    "Y_test_component3 = Component_test3['component']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>component</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>27396</th>\n",
       "      <td>trump warns hillary wants to abolish the secon...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49747</th>\n",
       "      <td>the 11 best laptops of 2016</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26513</th>\n",
       "      <td>team of grifters tim kaine reinforces crooked ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19619</th>\n",
       "      <td>investigation migrants smuggled into uk posing...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1073</th>\n",
       "      <td>a long way from mexico company bets china has ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   title  component\n",
       "27396  trump warns hillary wants to abolish the secon...          0\n",
       "49747                        the 11 best laptops of 2016          1\n",
       "26513  team of grifters tim kaine reinforces crooked ...          1\n",
       "19619  investigation migrants smuggled into uk posing...          1\n",
       "1073   a long way from mexico company bets china has ...          0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Component_train3.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>component</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>27594</th>\n",
       "      <td>clinton vp pick tim kaines islamist ties</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21699</th>\n",
       "      <td>report donald trump no show at colorado state ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40577</th>\n",
       "      <td>generous kidney donor triggers 6 transplants</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28470</th>\n",
       "      <td>zumwalt fifteen years after 911 what have we l...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24395</th>\n",
       "      <td>texas prisoners bust out of jail  to save jailer</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   title  component\n",
       "27594         clinton vp pick tim kaines islamist ties            1\n",
       "21699  report donald trump no show at colorado state ...          0\n",
       "40577       generous kidney donor triggers 6 transplants          0\n",
       "28470  zumwalt fifteen years after 911 what have we l...          1\n",
       "24395   texas prisoners bust out of jail  to save jailer          1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Component_test3.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Articles in Component  0\n",
      "                                                  title  component\n",
      "9830  rep pete roskam post ryancare congress must st...          0\n",
      "9850  bill gates calls for robot tax to offset jobs ...          0\n",
      "7399  antnio guterres pledges to help vulnerable as ...          0\n",
      "\n",
      "Articles in Component  1\n",
      "                                          title  component\n",
      "33160  the disease that could bankrupt medicare          1\n",
      "36485                      tim kaine fast facts          1\n",
      "34657                    kim jong il fast facts          1\n",
      "\n",
      "Articles in Component  2\n",
      "                                                   title  component\n",
      "47668  the mclaren 675lt is the hightech supercar for...          2\n",
      "46690         how to be assertive rather than aggressive          2\n",
      "10779    trumps h1b crackdown upsets chamber of commerce          2\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    print(\"\\nArticles in Component \", i)\n",
    "    title_index = titles1_by_component2.loc[:,i].sort_values(ascending=False)[0:3].index\n",
    "    print(Component_train3.loc[title_index,['title','component']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## This is noise so it is too few topics so a lot of different articles are put together   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The shape of Y_train_tfidf for articles titles is: (7500, 338978)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    5258\n",
       "1    1789\n",
       "2     453\n",
       "Name: component, dtype: int64"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('\\nThe shape of Y_train_tfidf for articles titles is:', Y_train_tfidf.shape)\n",
    "Component_train3.component.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Component 0 has the most articles       \n",
    "The articles can be seen as well distributed between the different Components. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top terms per components:\n",
      "\n",
      "tf_vectorizer on done in 0.19966499999999598 seconds\n",
      "\n",
      "Topics in Component  0\n",
      "['cruz', 'donald', 'gop', 'house', 'obama', 'police', 'report', 'says', 'trump', 'white']\n",
      "\n",
      "tf_vectorizer on done in 0.2535299999999978 seconds\n",
      "\n",
      "Topics in Component  1\n",
      "['campaign', 'clinton', 'facts', 'fast', 'hillary', 'new', 'sanders', 'state', 'years', 'york']\n",
      "\n",
      "tf_vectorizer on done in 0.2756569999999954 seconds\n",
      "\n",
      "Topics in Component  2\n",
      "['ban', 'donald', 'election', 'facebook', 'immigration', 'new', 'speech', 'trumps', 'twitter', 'wall']\n"
     ]
    }
   ],
   "source": [
    "tf_vectorizer = CountVectorizer(max_features=10,\n",
    "                                stop_words='english')\n",
    "t0 = time.clock()\n",
    "\n",
    "print(\"Top terms per components:\") \n",
    "for i in range(3):\n",
    "    tf = tf_vectorizer.fit_transform(Component_train3.loc[Component_train3['component'] == i,'title'])\n",
    "    print('\\ntf_vectorizer on done in '+'%s seconds'% (time.clock() - t0))\n",
    "    print(\"\\nTopics in Component \", i)\n",
    "    tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "    print(tf_feature_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Using Tensor Flow and Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras import optimizers\n",
    "\n",
    "# All parameter gradients will be clipped to\n",
    "# a maximum value of 0.5 and\n",
    "# a minimum value of -0.5.\n",
    "#sgd = optimizers.SGD(lr=0.01, clipvalue=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500 train samples\n",
      "2500 test samples\n"
     ]
    }
   ],
   "source": [
    "# Print sample sizes\n",
    "print(X_train_tfidf.shape[0], 'train samples')\n",
    "print(X_test_tfidf.shape[0], 'test samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using articles titles clustered Using LSA n = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 classes\n"
     ]
    }
   ],
   "source": [
    "# Convert class vectors to binary class matrices\n",
    "nb_classes = 10\n",
    "print(nb_classes, 'classes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y_train shape: (7500, 10)\n",
      "Y_test shape: (2500, 10)\n"
     ]
    }
   ],
   "source": [
    "Y_train_tf1 = keras.utils.to_categorical(Y_train_component10, nb_classes)\n",
    "Y_test_tf1 = keras.utils.to_categorical(Y_test_component10, nb_classes)\n",
    "\n",
    "\n",
    "print('Y_train shape:', Y_train_tf1.shape)\n",
    "print('Y_test shape:', Y_test_tf1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nb_filter = 64                   ##Always 2^x features\n",
    "nb_outputs = Y_train_tf1.shape[1]\n",
    "\n",
    "kernel_size = 3\n",
    "nb_samples = X_train_tfidf.shape[0]\n",
    "nb_features = X_train_tfidf.shape[1]\n",
    "newshape = (nb_features,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Transform Sparse matrix into array\n",
    "X1 = X_train_tfidf.toarray()\n",
    "X2 = X_test_tfidf.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# reshape Train data\n",
    "X_train_r = np.zeros((X_train_tfidf.shape[0], nb_features, 1))\n",
    "X_train_r[:, :, 0] = X1[:,:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# reshape Test data\n",
    "X_test_r = np.zeros((X_test_tfidf.shape[0], nb_features, 1))\n",
    "X_test_r[:, :, 0] = X2[:,:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Building the Model\n",
    "model = Sequential()\n",
    "# First convolutional layer, note the specification of shape\n",
    "model.add(Convolution1D(filters=nb_filter,kernel_size=kernel_size, \n",
    "                 activation='relu',\n",
    "                 input_shape=newshape))     \n",
    "model.add(Dropout(0.15))\n",
    "model.add(MaxPooling1D())\n",
    "model.add(Dropout(0.10))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(nb_outputs, activation='softmax'))\n",
    "\n",
    "#model.compile(loss='mse', optimizer='adam', metrics=['mae'])\n",
    "model.compile(loss=keras.losses.categorical_hinge, optimizer='sgd', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.fit(X_train_r, Y_train_tf1,\n",
    "          batch_size=64,\n",
    "          epochs=5,\n",
    "          verbose=1,\n",
    "          validation_data=(X_test_r, Y_test_tf1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "score = model.evaluate(X_test_r, Y_test_tf1, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ypred_10 = model.predict(X_test_r,\n",
    "                         batch_size=None,\n",
    "                         verbose=0,\n",
    "                         steps=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "confusion_matrix(Y_test_tf1.argmax(axis=1), ypred_10.argmax(axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "__Parameters:__ \n",
    " <img src=\"KerasCM10_1.png\"> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y_test_component10.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "code_folding": []
   },
   "source": [
    "#### Prediction on New Article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Transform Sparse matrix into array\n",
    "X3 = X_new_tfidf.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# reshape Test data\n",
    "X_new_r = np.zeros((X_new_tfidf.shape[0], nb_features, 1))\n",
    "X_new_r[:, :, 0] = X3[:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#from numpy import array\n",
    "# make a prediction\n",
    "y_new10 = model.predict(X_new_r, \n",
    "                         batch_size=None,\n",
    "                         verbose=0,\n",
    "                         steps=None)\n",
    "# show the inputs and predicted outputs\n",
    "print(y_new10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(X_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[0.09998474 0.09997356 0.10000128 0.10001415 0.09999078 0.10004263\n",
    "  0.10000007 0.0999989  0.09999607 0.09999783]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "model.add(Convolution1D(filters=nb_filter,kernel_size=kernel_size, \n",
    "                 activation='relu',\n",
    "                 input_shape=newshape))     \n",
    "model.add(Dropout(0.25))\n",
    "model.add(MaxPooling1D())\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(nb_outputs, activation='linear'))\n",
    "\n",
    "model.compile(loss='mse', optimizer='adam', metrics=['mae'])\n",
    "\n",
    "\n",
    "__conv1d(64)   \n",
    "Dense(128)   \n",
    "batch_size=64,   \n",
    "epochs=5,   \n",
    "verbose=1,__   \n",
    "\n",
    "\n",
    "Train on 7500 samples, validate on 2500 samples     \n",
    "Epoch 1/5\n",
    "7500/7500 [==============================] - 14115s 2s/step - loss: 0.1008 - mean_absolute_error: 0.1556 - val_loss: 0.0744 - val_mean_absolute_error: 0.1464   \n",
    "Epoch 2/5\n",
    "7500/7500 [==============================] - 6252s 834ms/step - loss: 0.0689 - mean_absolute_error: 0.1549 - val_loss: 0.0729 - val_mean_absolute_error: 0.1461    \n",
    "Epoch 3/5\n",
    "7500/7500 [==============================] - 7936s 1s/step - loss: 0.0544 - mean_absolute_error: 0.1435 - val_loss: 0.0748 - val_mean_absolute_error: 0.1480    \n",
    "Epoch 4/5\n",
    "7500/7500 [==============================] - 8315s 1s/step - loss: 0.0442 - mean_absolute_error: 0.1328 - val_loss: 0.0759 - val_mean_absolute_error: 0.1485    \n",
    "Epoch 5/5\n",
    "7500/7500 [==============================] - 5221s 696ms/step - loss: 0.0386 - mean_absolute_error: 0.1256 - val_loss: 0.0763 - val_mean_absolute_error: 0.1485   \n",
    "\n",
    "Test loss: 0.07626755193471908    \n",
    "Test accuracy: 0.1485029278039932     \n",
    "\n",
    "\n",
    "        \n",
    "__conv1d(128)   \n",
    "Dense(64)   \n",
    "batch_size=128,   \n",
    "epochs=2,   \n",
    "verbose=1,__   \n",
    "Train on 7500 samples, validate on 2500 samples\n",
    "Epoch 1/2\n",
    "\n",
    "Nothing was able to move for this one.\n",
    "\n",
    "\n",
    "\n",
    "__conv1d(64)   \n",
    "Dense(64)   \n",
    "batch_size=64,   \n",
    "epochs=2,   \n",
    "verbose=1,__\n",
    "\n",
    "Train on 7500 samples, validate on 2500 samples   \n",
    "Epoch 1/2   \n",
    "7500/7500 [==============================] - 2579s 344ms/step - loss: 0.1041 - mean_absolute_error: 0.1555 - val_loss: 0.0681 - val_mean_absolute_error: 0.1364    \n",
    "Epoch 2/2    \n",
    "7500/7500 [==============================] - 1945s 259ms/step - loss: 0.0658 - mean_absolute_error: 0.1468 - val_loss: 0.0648 - val_mean_absolute_error: 0.1308   \n",
    "<keras.callbacks.History at 0x12a6b1a58>   \n",
    "Test loss: 0.06476313845515251   \n",
    "Test accuracy: 0.13079609287977217    \n",
    "\n",
    "  \n",
    "\n",
    "__conv1d(64)   \n",
    "Dense(64)   \n",
    "batch_size=128,   \n",
    "epochs=2,   \n",
    "verbose=1,__\n",
    "          \n",
    "Train on 7500 samples, validate on 2500 samples   \n",
    "Epoch 1/2    \n",
    "7500/7500 [==============================] - 2050s 273ms/step - loss: 0.0976 - mean_absolute_error: 0.1378 - val_loss: 0.0745 - val_mean_absolute_error: 0.1292   \n",
    "Epoch 2/2    \n",
    "7500/7500 [==============================] - 2097s 280ms/step - loss: 0.0697 - mean_absolute_error: 0.1516 - val_loss: 0.0653 - val_mean_absolute_error: 0.1319    \n",
    "<keras.callbacks.History at 0x12f2af518>    \n",
    "Test loss: 0.06534156835079193    \n",
    "Test accuracy: 0.1318500873565674   \n",
    "\n",
    "\n",
    "\n",
    "__loss=keras.losses.categorical_crossentropy, optimizer='sgd', metrics=['accuracy']___\n",
    "\n",
    "__conv1d(64)   \n",
    "Dense(64)   \n",
    "batch_size=64,   \n",
    "epochs=5,   \n",
    "verbose=1,__\n",
    "\n",
    "Train on 7500 samples, validate on 2500 samples   \n",
    "Epoch 1/5    \n",
    "7500/7500 [==============================] - 2245s 299ms/step - loss: 8.6386 - acc: 0.2128 - val_loss: 9.8707 - val_acc: 0.1280    \n",
    "Epoch 2/5    \n",
    "7500/7500 [==============================] - 1807s 241ms/step - loss: 8.5938 - acc: 0.2019 - val_loss: 9.8707 - val_acc: 0.3100    \n",
    "Epoch 3/5     \n",
    "7500/7500 [==============================] - 1655s 221ms/step - loss: 8.6944 - acc: 0.2169 - val_loss: 9.8707 - val_acc: 0.0976        \n",
    "Epoch 4/5     \n",
    "7500/7500 [==============================] - 1741s 232ms/step - loss: 8.5582 - acc: 0.2005 - val_loss: 9.9094 - val_acc: 0.0976         \n",
    "Epoch 5/5     \n",
    "7500/7500 [==============================] - 1943s 259ms/step - loss: 9.0115 - acc: 0.2031 - val_loss: 9.8707 - val_acc: 0.3100        \n",
    "<keras.callbacks.History at 0x12f719748>         \n",
    "Test loss: 9.870721658325195     \n",
    "Test accuracy: 0.31    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Train on 7500 samples, validate on 2500 samples    \n",
    "Epoch 1/5      \n",
    "7500/7500 [==============================] - 3048s 406ms/step - loss: 1.0001 - acc: 0.2711 - val_loss: 1.0000 - val_acc: 0.3168     \n",
    "Epoch 2/5       \n",
    "7500/7500 [==============================] - 3020s 403ms/step - loss: 1.0000 - acc: 0.2708 - val_loss: 1.0000 - val_acc: 0.3416      \n",
    "Epoch 3/5      \n",
    "7500/7500 [==============================] - 2990s 399ms/step - loss: 1.0000 - acc: 0.2696 - val_loss: 1.0000 - val_acc: 0.3496     \n",
    "Epoch 4/5      \n",
    "7500/7500 [==============================] - 2075s 277ms/step - loss: 1.0000 - acc: 0.2692 - val_loss: 1.0000 - val_acc: 0.3264      \n",
    "Epoch 5/5      \n",
    "7500/7500 [==============================] - 2154s 287ms/step - loss: 1.0000 - acc: 0.2636 - val_loss: 1.0000 - val_acc: 0.3492      \n",
    "<keras.callbacks.History at 0x1263824e0>    \n",
    "Test loss: 1.0000195775985719     \n",
    "Test accuracy: 0.3492  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Train on 7500 samples, validate on 2500 samples     \n",
    "Epoch 1/5      \n",
    "7500/7500 [==============================] - 3048s 406ms/step - loss: 1.0001 - acc: 0.2711 - val_loss: 1.0000 - val_acc: 0.3168     \n",
    "Epoch 2/5      \n",
    "7500/7500 [==============================] - 3020s 403ms/step - loss: 1.0000 - acc: 0.2708 - val_loss: 1.0000 - val_acc: 0.3416      \n",
    "Epoch 3/5     \n",
    "7500/7500 [==============================] - 2990s 399ms/step - loss: 1.0000 - acc: 0.2696 - val_loss: 1.0000 - val_acc: 0.3496     \n",
    "Epoch 4/5     \n",
    "7500/7500 [==============================] - 2075s 277ms/step - loss: 1.0000 - acc: 0.2692 - val_loss: 1.0000 - val_acc: 0.3264     \n",
    "Epoch 5/5     \n",
    "7500/7500 [==============================] - 2154s 287ms/step - loss: 1.0000 - acc: 0.2636 - val_loss: 1.0000 - val_acc: 0.3492     \n",
    "<keras.callbacks.History at 0x1263824e0>  \n",
    "\n",
    "Test loss: 1.0000195775985719     \n",
    "Test accuracy: 0.3492    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TRy this model with \n",
    "__conv1d(64)   \n",
    "Dense(64)   \n",
    "batch_size=64,   \n",
    "epochs=2,   \n",
    "verbose=1,__   with 5 epochs "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using articles titles clustered Using LSA n = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 classes\n"
     ]
    }
   ],
   "source": [
    "# Convert class vectors to binary class matrices\n",
    "nb_classes2 = 3\n",
    "print(nb_classes2, 'classes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y_train shape: (7500, 3)\n",
      "Y_test shape: (2500, 3)\n"
     ]
    }
   ],
   "source": [
    "Y_train_tf3 = keras.utils.to_categorical(Y_train_component3, nb_classes2)\n",
    "Y_test_tf3 = keras.utils.to_categorical(Y_test_component3, nb_classes2)\n",
    "\n",
    "\n",
    "print('Y_train shape:', Y_train_tf3.shape)\n",
    "print('Y_test shape:', Y_test_tf3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nb_filter = 64                   ##Always 2^x features\n",
    "nb_outputs = Y_train_tf3.shape[1]\n",
    "\n",
    "kernel_size = 3\n",
    "nb_samples = X_train_tfidf.shape[0]\n",
    "nb_features = X_train_tfidf.shape[1]\n",
    "newshape = (nb_features,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Transform Sparse matrix into array\n",
    "X1 = X_train_tfidf.toarray()\n",
    "X2 = X_test_tfidf.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# reshape Train data\n",
    "X_train_r = np.zeros((X_train_tfidf.shape[0], nb_features, 1))\n",
    "X_train_r[:, :, 0] = X1[:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# reshape Test data\n",
    "X_test_r = np.zeros((X_test_tfidf.shape[0], nb_features, 1))\n",
    "X_test_r[:, :, 0] = X2[:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Building the Model\n",
    "model = Sequential()\n",
    "# First convolutional layer, note the specification of shape\n",
    "model.add(Convolution1D(filters=nb_filter,kernel_size=kernel_size, \n",
    "                 activation='relu',\n",
    "                 input_shape=newshape))     \n",
    "model.add(Dropout(0.15))\n",
    "\n",
    "model.add(MaxPooling1D())\n",
    "model.add(Dropout(0.10))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(nb_outputs, activation='softmax'))\n",
    "\n",
    "#model.compile(loss='mse', optimizer='adam', metrics=['mae'])\n",
    "#model.compile(loss=keras.losses.categorical_crossentropy, optimizer='sgd', metrics=['accuracy'])\n",
    "model.compile(loss=keras.losses.categorical_hinge, optimizer='sgd', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.fit(X_train_r, Y_train_tf3,\n",
    "          batch_size=64,\n",
    "          epochs=5,\n",
    "          verbose=1,\n",
    "          validation_data=(X_test_r, Y_test_tf3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "score = model.evaluate(X_test_r, Y_test_tf3, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ypred_3 = model.predict(X_test_r, \n",
    "                         batch_size=None,\n",
    "                         verbose=0,\n",
    "                         steps=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "confusion_matrix(Y_test_tf3.argmax(axis=1), ypred_3.argmax(axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " __Parameters:__ \n",
    " <img src=\"KerasCM3_1.png\"> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y_test_component3.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "code_folding": []
   },
   "source": [
    "#### Prediction on New Article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Transform Sparse matrix into array\n",
    "X3 = X_new_tfidf.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# reshape Test data\n",
    "X_new_r = np.zeros((X_new_tfidf.shape[0], nb_features, 1))\n",
    "X_new_r[:, :, 0] = X3[:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#from numpy import array\n",
    "# make a prediction\n",
    "y_new3 = model.predict(X_new_r, \n",
    "                         batch_size=None,\n",
    "                         verbose=0,\n",
    "                         steps=None)\n",
    "# show the inputs and predicted outputs\n",
    "print(y_new3)\n",
    "print(X_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[0.33624652 0.33195335 0.33180022]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "Counter(Y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__conv1d(64)   \n",
    "Dense(64)   \n",
    "batch_size=64,   \n",
    "epochs=10,   \n",
    "verbose=1__\n",
    "\n",
    "\n",
    "\n",
    "Train on 7500 samples, validate on 2500 samples    \n",
    "Epoch 1/10      \n",
    "7500/7500 [==============================] - 2578s 344ms/step - loss: 0.2441 - mean_absolute_error: 0.3774 - val_loss: 0.1695 - val_mean_absolute_error: 0.3376     \n",
    "Epoch 2/10     \n",
    "7500/7500 [==============================] - 3057s 408ms/step - loss: 0.1537 - mean_absolute_error: 0.3079 - val_loss: 0.1679 - val_mean_absolute_error: 0.3218    \n",
    "Epoch 3/10       \n",
    "7500/7500 [==============================] - 2642s 352ms/step - loss: 0.1189 - mean_absolute_error: 0.2619 - val_loss: 0.1746 - val_mean_absolute_error: 0.3171        \n",
    "Epoch 4/10      \n",
    "7500/7500 [==============================] - 3293s 439ms/step - loss: 0.0941 - mean_absolute_error: 0.2274 - val_loss: 0.1807 - val_mean_absolute_error: 0.3212        \n",
    "Epoch 5/10         \n",
    "7500/7500 [==============================] - 2412s 322ms/step - loss: 0.0780 - mean_absolute_error: 0.2056 - val_loss: 0.1841 - val_mean_absolute_error: 0.3201        \n",
    "Epoch 6/10     \n",
    "7500/7500 [==============================] - 2801s 373ms/step - loss: 0.0689 - mean_absolute_error: 0.1919 - val_loss: 0.1852 - val_mean_absolute_error: 0.3236        \n",
    "Epoch 7/10      \n",
    "7500/7500 [==============================] - 2900s 387ms/step - loss: 0.0628 - mean_absolute_error: 0.1828 - val_loss: 0.1854 - val_mean_absolute_error: 0.3246         \n",
    "Epoch 8/10     \n",
    "7500/7500 [==============================] - 2455s 327ms/step - loss: 0.0577 - mean_absolute_error: 0.1755 - val_loss: 0.1844 - val_mean_absolute_error: 0.3250       \n",
    "Epoch 9/10    \n",
    "7500/7500 [==============================] - 7130s 951ms/step - loss: 0.0529 - mean_absolute_error: 0.1688 - val_loss: 0.1836 - val_mean_absolute_error: 0.3257\n",
    "Epoch 10/10           \n",
    "7500/7500 [==============================] - 6540s 872ms/step - loss: 0.0511 - mean_absolute_error: 0.1658 - val_loss: 0.1831 - val_mean_absolute_error: 0.3279    \n",
    "<keras.callbacks.History at 0x12340d048>    \n",
    "Test loss: 0.18309213342666625    \n",
    "Test accuracy: 0.3279262701034546   \n",
    "\n",
    "\n",
    "__conv1d(64)   \n",
    "Dense(64)   \n",
    "batch_size=64,   \n",
    "epochs=5,   \n",
    "verbose=1__\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Train on 7500 samples, validate on 2500 samples   \n",
    "Epoch 1/5     \n",
    "7500/7500 [==============================] - 2336s 311ms/step - loss: 1.3274 - acc: 0.4679 - val_loss: 1.0016 - val_acc: 0.4748      \n",
    "Epoch 2/5\n",
    "7500/7500 [==============================] - 2130s 284ms/step - loss: 1.0227 - acc: 0.4949 - val_loss: 0.9976 - val_acc: 0.4748      \n",
    "Epoch 3/5      \n",
    "7500/7500 [==============================] - 2083s 278ms/step - loss: 1.0066 - acc: 0.4963 - val_loss: 0.9994 - val_acc: 0.4748      \n",
    "Epoch 4/5      \n",
    "7500/7500 [==============================] - 2169s 289ms/step - loss: 1.0056 - acc: 0.4963 - val_loss: 0.9973 - val_acc: 0.4748      \n",
    "Epoch 5/5      \n",
    "7500/7500 [==============================] - 2261s 301ms/step - loss: 1.0189 - acc: 0.4963 - val_loss: 0.9968 - val_acc: 0.4748     \n",
    "<keras.callbacks.History at 0x125c3c710>    \n",
    "\n",
    "Test loss: 0.9967564933776856      \n",
    "Test accuracy: 0.4748     \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using articles titles clustered Using LSA n = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convert class vectors to binary class matrices\n",
    "nb_classes1 = 20\n",
    "print(nb_classes1, 'classes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Y_train_tf2 = keras.utils.to_categorical(Y_train_component20, nb_classes1)\n",
    "Y_test_tf2 = keras.utils.to_categorical(Y_test_component20, nb_classes1)\n",
    "\n",
    "\n",
    "print('Y_train shape:', Y_train_tf2.shape)\n",
    "print('Y_test shape:', Y_test_tf2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nb_filter = 64                   ##Always 2^x features\n",
    "nb_outputs = Y_train_tf2.shape[1]\n",
    "\n",
    "kernel_size = 3\n",
    "nb_samples = X_train_tfidf.shape[0]\n",
    "nb_features = X_train_tfidf.shape[1]\n",
    "newshape = (nb_features,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Transform Sparse matrix into array\n",
    "X1 = X_train_tfidf.toarray()\n",
    "X2 = X_test_tfidf.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# reshape Train data\n",
    "X_train_r = np.zeros((X_train_tfidf.shape[0], nb_features, 1))\n",
    "X_train_r[:, :, 0] = X1[:,:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# reshape Test data\n",
    "X_test_r = np.zeros((X_test_tfidf.shape[0], nb_features, 1))\n",
    "X_test_r[:, :, 0] = X2[:,:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Building the Model\n",
    "model = Sequential()\n",
    "# First convolutional layer, note the specification of shape\n",
    "model.add(Convolution1D(filters=nb_filter,kernel_size=kernel_size, \n",
    "                 activation='relu',\n",
    "                 input_shape=newshape))     \n",
    "model.add(Dropout(0.15))\n",
    "model.add(MaxPooling1D())\n",
    "model.add(Dropout(0.10))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64, activation='softmax'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(nb_outputs, activation='softmax'))\n",
    "\n",
    "#model.compile(loss='mse', optimizer='adam', metrics=['mae'])\n",
    "model.compile(loss=keras.losses.categorical_hinge, optimizer='sgd', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.fit(X_train_r, Y_train_tf2,\n",
    "          batch_size=64,\n",
    "          epochs=5,\n",
    "          verbose=1,\n",
    "          validation_data=(X_test_r, Y_test_tf2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "score = model.evaluate(X_test_r, Y_test_tf2, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ypred_20 = model.predict(X_test_r, \n",
    "                         batch_size=None,\n",
    "                         verbose=0,\n",
    "                         steps=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "confusion_matrix(Y_test_tf2.argmax(axis=1), ypred_20.argmax(axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "__Parameters:__ \n",
    " <img src=\"KerasCM20_1.png\"> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y_test_component20.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "code_folding": []
   },
   "source": [
    "#### Prediction on New Article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Transform Sparse matrix into array\n",
    "X3 = X_new_tfidf.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# reshape Test data\n",
    "X_new_r = np.zeros((X_new_tfidf.shape[0], nb_features, 1))\n",
    "X_new_r[:, :, 0] = X3[:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#from numpy import array\n",
    "# make a prediction\n",
    "y_new20 = model.predict(X_new_r, \n",
    "                         batch_size=None,\n",
    "                         verbose=0,\n",
    "                         steps=None)\n",
    "# show the inputs and predicted outputs\n",
    "print(y_new20)\n",
    "print(X_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "[[0.04999167 0.05000684 0.05000392 0.05000376 0.04997295 0.05000558\n",
    "  0.04999379 0.0500024  0.0500035  0.05000408 0.04999643 0.05000095\n",
    "  0.0499965  0.05000525 0.04999898 0.05000291 0.05000332 0.05001199\n",
    "  0.05000304 0.04999211]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model.add(Convolution1D(filters=nb_filter,kernel_size=kernel_size, \n",
    "                 activation='relu',\n",
    "                 input_shape=newshape))     \n",
    "model.add(Dropout(0.25))\n",
    "model.add(MaxPooling1D())\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(nb_outputs, activation='linear'))\n",
    "\n",
    "model.compile(loss='mse', optimizer='adam', metrics=['mae'])\n",
    "\n",
    "__conv1d(64)   \n",
    "Dense(128)   \n",
    "batch_size=64,   \n",
    "epochs=2,   \n",
    "verbose=1,__   \n",
    "\n",
    "Train on 7500 samples, validate on 2500 samples   \n",
    "Epoch 1/2    \n",
    "7500/7500 [==============================] - 4167s 556ms/step - loss: 0.0463 - mean_absolute_error: 0.0865 - val_loss: 0.0399 - val_mean_absolute_error: 0.0798    \n",
    "Epoch 2/2    \n",
    "7500/7500 [==============================] - 4581s 611ms/step - loss: 0.0369 - mean_absolute_error: 0.0936 - val_loss: 0.0389 - val_mean_absolute_error: 0.0817    \n",
    "<keras.callbacks.History at 0x1299f5b38>   \n",
    "   \n",
    "Test loss: 0.0389187620639801   \n",
    "Test accuracy: 0.0816848998427391\n",
    "\n",
    "\n",
    "\n",
    "__conv1d(64)   \n",
    "Dense(64)   \n",
    "batch_size=128,   \n",
    "epochs=2,   \n",
    "verbose=1,__     \n",
    "\n",
    "\n",
    "Train on 7500 samples, validate on 2500 samples   \n",
    "Epoch 1/2    \n",
    "7500/7500 [==============================] - 2718s 362ms/step - loss: 0.0475 - mean_absolute_error: 0.0848 - val_loss: 0.0425 - val_mean_absolute_error: 0.0824     \n",
    "Epoch 2/2    \n",
    "7500/7500 [==============================] - 2151s 287ms/step - loss: 0.0408 - mean_absolute_error: 0.0962 - val_loss: 0.0400 - val_mean_absolute_error: 0.0832    \n",
    "<keras.callbacks.History at 0x12d58a0f0>    \n",
    "\n",
    "Test loss: 0.040040286999940874    \n",
    "Test accuracy: 0.08322641594409942  \n",
    "\n",
    "__conv1d(64)   \n",
    "Dense(64)   \n",
    "batch_size=64,   \n",
    "epochs=5,   \n",
    "verbose=1,__   \n",
    "\n",
    "\n",
    "Train on 7500 samples, validate on 2500 samples    \n",
    "Epoch 1/5    \n",
    "7500/7500 [==============================] - 2219s 296ms/step - loss: 1.0001 - acc: 0.0985 - val_loss: 1.0000 - val_acc: 0.1220    \n",
    "Epoch 2/5     \n",
    "7500/7500 [==============================] - 2002s 267ms/step - loss: 1.0000 - acc: 0.1035 - val_loss: 1.0000 - val_acc: 0.1288    \n",
    "Epoch 3/5    \n",
    "7500/7500 [==============================] - 2002s 267ms/step - loss: 1.0000 - acc: 0.1069 - val_loss: 1.0000 - val_acc: 0.1216    \n",
    "Epoch 4/5      \n",
    "7500/7500 [==============================] - 1978s 264ms/step - loss: 1.0000 - acc: 0.1035 - val_loss: 1.0000 - val_acc: 0.0480     \n",
    "Epoch 5/5     \n",
    "7500/7500 [==============================] - 1914s 255ms/step - loss: 1.0000 - acc: 0.0988 - val_loss: 1.0000 - val_acc: 0.1320      \n",
    "\n",
    "Test loss: 1.000010488128662    \n",
    "Test accuracy: 0.132     "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "273px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
